# Table of Contents
- [Feature Scaling](#feature-scaling)
- [Feature Engineering](#feature-engineering)
- [Ensemble Learning](#ensemble-learning)
  - [Types](#types)
- [Bagging/Bootstrap Aggregating](#baggingbootstrap-aggregating)
  - [How it works](#how-it-works)
    - [Bootstrap Sampling](#bootstrap-sampling)
    - [Model Training](#model-training)
    - [Predictions](#predictions)
    - [Aggregation](#aggregation)
- [Random Forest](#random-forest)
- [How Weights are Increased in Boosting](#how-weights-are-increased-in-boosting)


## Feature Scaling
Feature scaling is the process of normalizing or standardizing the range of features in a dataset so they all fit within a similar scale. This helps models learn more effectively and can lead to faster convergence and better performance.

### Why Do We Need Feature Scaling?
- **Algorithm Sensitivity to Scale:** Some algorithms, like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM), are distance-based. If features are on different scales, those with larger ranges dominate distance calculations.

- **Gradient Descent Optimization:** Algorithms like linear regression and neural networks use gradient descent, which works faster when features are on a similar scale. Without scaling, the gradient descent might take longer to converge, or it may even fail to converge properly.

- **Improved Accuracy and Interpretability:** Standardizing data can improve model accuracy and make features comparable.

### Common Methods of Feature Scaling
Different techniques are used depending on the problem and the type of data:

1. **Min-Max Scaling (Normalization)**

    - Formula: $\[ x' = \frac{(x - x_{min})}{(x_{max} - x_{min})} \]$
    - Range: Scales the data to a fixed range, usually [0, 1] or [-1, 1].
    - Use Case: Suitable when the data does not have extreme outliers.       

2. **Standardization (Z-score Normalization)**

    - Formula: $\[ x' = \frac{(x - \mu)}{\sigma} \]$ 
    - Range: Transforms data to have a mean of 0 and a standard deviation of 1. 
    - Use Case: Useful for algorithms that assume normally distributed data. Commonly used in algorithms like SVM, logistic regression, and neural networks.

3. **Robust Scaling**

    - Formula: $\[ x' = \frac{(x - \text{median})}{\text{IQR}} \]$  
    - Range: Uses the median and interquartile range (IQR) instead of mean and standard deviation.  
    - Use Case: Best for data with outliers, as it's less sensitive to them.

### How to Implement Feature Scaling?
Libraries like Python’s scikit-learn provide easy-to-use scaling functions.

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
scaled_data = min_max_scaler.fit_transform(data)

# Standardization
standard_scaler = StandardScaler()
standardized_data = standard_scaler.fit_transform(data)

# Robust Scaling
robust_scaler = RobustScaler()
robust_scaled_data = robust_scaler.fit_transform(data)
```

### Practical Tips
- **Fit on Train, Transform on Test:** To avoid data leakage, always fit the scaler on the training data and then use it to transform the test data.
- **Avoid Scaling Target Variable:** Usually, feature scaling applies only to input features, not the target variable. However, for regression problems, scaling the target variable can sometimes help.
- **Pipeline Integration:** Integrate feature scaling into your machine learning pipeline to make sure it’s applied consistently.

## Feature Engineering
- Using intuition to design new features, by transforming or combining original features.

## Ensemble Learning
- It uses multiple learning algorithms to obtain better predictive performance.
- Giving dataset to multiple models (KNN, SVM, Logistic Regression, etc.)
- Types: Bootstrap aggregating, Boosting, Stacking, Voting

## Bagging/Bootstrap Aggregating
- It is an ensemble machine learning technique designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also helps reduce overfitting.

### How it works:
- **Bootstrap Sampling**: Multiple subsets of the original dataset are created by randomly sampling with replacement. This means some observations may be repeated in each subset.
- **Model Training**: A separate model is trained on each bootstrap sample.
- **Predictions**: Each model makes its own prediction on new data.
- **Aggregation**: The predictions from all models are combined to create a final prediction. This could be through majority voting (for classification) or averaging (for regression).

- `Random Forest` is a popular algorithm that uses bagging with decision trees.

### How Weights are Increased in Boosting:
1. **Initialize weights**: If there are 5 total rows, then each row has a weight of \( \frac{1}{5} \).
2. **Train weak learner 1 (model)**.
3. **Calculate total error**: Sum the errors of misclassified rows.
4. **Calculate weak learner weight (α)**: 1/2 ln(1-total error/total error)
5. **Update instance weights**: old weight * e^α
6. **Repeat from step 2** until the desired number of weak learners is trained.
