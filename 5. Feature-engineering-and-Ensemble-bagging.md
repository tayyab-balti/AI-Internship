# Table of Contents
- [Feature Scaling](#feature-scaling)
- [Feature Engineering](#feature-engineering)
- [Ensemble Learning](#ensemble-learning)
  - [Types](#types)
- [Bagging/Bootstrap Aggregating](#baggingbootstrap-aggregating)
  - [How it works](#how-it-works)
    - [Bootstrap Sampling](#bootstrap-sampling)
    - [Model Training](#model-training)
    - [Predictions](#predictions)
    - [Aggregation](#aggregation)
- [Random Forest](#random-forest)
- [How Weights are Increased in Boosting](#how-weights-are-increased-in-boosting)


## Feature Scaling
Feature scaling is the process of normalizing or standardizing the range of features in a dataset so they all fit within a similar scale. This helps models learn more effectively and can lead to faster convergence and better performance.

### Why Do We Need Feature Scaling?
- **Algorithm Sensitivity to Scale:** Some algorithms, like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM), are distance-based. If features are on different scales, those with larger ranges dominate distance calculations.

- **Gradient Descent Optimization:** Algorithms like linear regression and neural networks use gradient descent, which works faster when features are on a similar scale. Without scaling, the gradient descent might take longer to converge, or it may even fail to converge properly.

- **Improved Accuracy and Interpretability:** Standardizing data can improve model accuracy and make features comparable.

### Common Methods of Feature Scaling
Different techniques are used depending on the problem and the type of data:

1. **Min-Max Scaling (Normalization)**

    - Formula: $\[ x' = \frac{(x - x_{min})}{(x_{max} - x_{min})} \]$
    - Range: Scales the data to a fixed range, usually [0, 1] or [-1, 1].
    - Use Case: Suitable when the data does not have extreme outliers.       

2. **Standardization (Z-score Normalization)**

    - Formula: $\[ x' = \frac{(x - \mu)}{\sigma} \]$ 
    - Range: Transforms data to have a mean of 0 and a standard deviation of 1. 
    - Use Case: Useful for algorithms that assume normally distributed data. Commonly used in algorithms like SVM, logistic regression, and neural networks.

3. **Robust Scaling**

    - Formula: $\[ x' = \frac{(x - \text{median})}{\text{IQR}} \]$  
    - Range: Uses the median and interquartile range (IQR) instead of mean and standard deviation.  
    - Use Case: Best for data with outliers, as it's less sensitive to them.

### How to Implement Feature Scaling?
Libraries like Python’s scikit-learn provide easy-to-use scaling functions.

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
scaled_data = min_max_scaler.fit_transform(data)

# Standardization
standard_scaler = StandardScaler()
standardized_data = standard_scaler.fit_transform(data)

# Robust Scaling
robust_scaler = RobustScaler()
robust_scaled_data = robust_scaler.fit_transform(data)
```

### Practical Tips
- **Fit on Train, Transform on Test:** To avoid data leakage, always fit the scaler on the training data and then use it to transform the test data.
- **Avoid Scaling Target Variable:** Usually, feature scaling applies only to input features, not the target variable. However, for regression problems, scaling the target variable can sometimes help.
- **Pipeline Integration:** Integrate feature scaling into your machine learning pipeline to make sure it’s applied consistently.

## Feature Engineering
It is the process of transforming raw data into meaningful features that better represent the underlying problem to predictive models, improving model accuracy and performance.

### Why Feature Engineering?
- **Enhances Model Performance:** Better features often lead to better accuracy.
- **Improves Interpretability:** Helps make predictions more understandable.
- **Handles Data Limitations:** Extracts useful signals from limited or raw data.

### Types of Feature Engineering
1. Feature Transformation
  - **Scaling/Normalization:** Rescales data, especially for distance-based models.
  - **Log Transformation:** Reduces skew in data with large outliers.
  - **Polynomial Features:** Captures interactions by adding feature powers (e.g, $x^2$ )

2. Feature Creation
  - **Date-Time Features:** Extract day, month, season, etc., from timestamps.
  - **Binning/Discretization:** Converts continuous data to bins (e.g., age groups).
  - **One-Hot Encoding:** Converts categories to binary columns for categorical data.

3. Feature Aggregation
  - **Grouping:** Summarizes data (e.g., average purchase per customer).
  - **Rolling Statistics:** Adds moving averages for time-series data.

4. Feature Encoding
  - **Label Encoding:** Assigns each category a unique integer (suitable for ordinal data).
  - **One-Hot Encoding:** Transforms categories into binary columns.
  - **Target Encoding:** Replaces categories with the target's average value for the category.

### Feature Selection Techniques
- **Filter Methods:** Uses statistical tests (e.g., correlation, chi-square) to rank features.
- **Wrapper Methods:** Evaluates feature combinations (e.g., recursive elimination).
- **Embedded Methods:** Uses model-based selection (e.g., LASSO regression).

### Implementing in Python

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Scaling and Standardizing
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[['feature1', 'feature2']])

# Encoding categorical data
encoder = OneHotEncoder()
encoded_data = encoder.fit_transform(df[['categorical_feature']])

# Creating new features from datetime
df['day_of_week'] = df['date_column'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)
```

### Workflow
1. Data Understanding
2. Missing Value Treatment
3. Encoding
4. Scaling
5. Feature Creation
6. Feature Selection
7. Model Training
8. Evaluation


## Ensemble Learning
- It uses multiple learning algorithms to obtain better predictive performance.
- Giving dataset to multiple models (KNN, SVM, Logistic Regression, etc.)
- Types: Bootstrap aggregating, Boosting, Stacking, Voting

## Bagging/Bootstrap Aggregating
- It is an ensemble machine learning technique designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also helps reduce overfitting.

### How it works:
- **Bootstrap Sampling**: Multiple subsets of the original dataset are created by randomly sampling with replacement. This means some observations may be repeated in each subset.
- **Model Training**: A separate model is trained on each bootstrap sample.
- **Predictions**: Each model makes its own prediction on new data.
- **Aggregation**: The predictions from all models are combined to create a final prediction. This could be through majority voting (for classification) or averaging (for regression).

- `Random Forest` is a popular algorithm that uses bagging with decision trees.

### How Weights are Increased in Boosting:
1. **Initialize weights**: If there are 5 total rows, then each row has a weight of \( \frac{1}{5} \).
2. **Train weak learner 1 (model)**.
3. **Calculate total error**: Sum the errors of misclassified rows.
4. **Calculate weak learner weight (α)**: 1/2 ln(1-total error/total error)
5. **Update instance weights**: old weight * e^α
6. **Repeat from step 2** until the desired number of weak learners is trained.
