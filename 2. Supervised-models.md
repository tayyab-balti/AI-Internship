# Table of Contents

- [Linear Regression](#linear-regression)
  - [Multiple Linear Regression](#multiple-linear-regression)
  - [Cost Function](#cost-function)
  - [Gradient Descent](#gradient-descent)
  - [Polynomial Regression](#polynomial-regression)
- [Logistic Regression](#logistic-regression)
- [K Nearest Neighbors (KNN)](#k-nearest-neighbors-knn)
- [Decision Trees](#decision-trees)
- [Random Forest](#random-forest)
- [Naive Bayes Classification](#naive-bayes-classification)
- [Support Vector Machines (SVM)](#support-vector-machines-svm)


## Linear Regression
It is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables.

`Real-life scenario:` Predicting house prices based on square footage.

![ML Linear Regression](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/C1_W1_L3_S1_Lecture_b.png)

### Multiple Linear Regression
Involves multiple independent variables (X‚ÇÅ, X‚ÇÇ, ... X‚Çô) and one dependent variable (Y).

`Equation:`  $f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot  \mathbf{x}^{(i)} + b$
             or Y = w‚ÇÅX‚ÇÅ + w‚ÇÇX‚ÇÇ + ... + w‚ÇôX‚Çô + b

Where:
- **Y**: Dependent variable
- **X**: Independent variable
- **w**: Slope/Coefficient (shows how much `y` changes with one-unit change in `X`)
- **b**: Y-intercept (value of `y` when `X` is 0)

```
- In ML, we write the equation for a linear regression model as follows:
```
![ML Linear Regression Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/ml_linear_equation.png)

### Visualization
![Linear Regression Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/multiple-linear-regression.png)

`Real-life scenario:` Predicting a car's fuel efficiency based on its weight, horsepower, and age.

### Code
```python
import pandas as pd
from sklearn.linear_model import LinearRegression

# Sample data
data = {'Weight': [1500, 1600, 1700, 1800, 1900],
        'Horsepower': [130, 140, 150, 160, 170],
        'Age': [5, 4, 3, 2, 1],
        'FuelEfficiency': [30, 28, 27, 25, 24]}
df = pd.DataFrame(data)

# Independent (X) and Dependent (Y) variables
X = df[['Weight', 'Horsepower', 'Age']]
y = df['FuelEfficiency']

# Initialize and fit the model
model = LinearRegression()
model.fit(X, y)

# Coefficients and Intercept
print(f"Coefficients: {model.coef_}, Intercept: {model.intercept_}")

# Predicting fuel efficiency for a car with 1600kg, 145HP, and 2 years old
predicted_efficiency = model.predict([[1600, 145, 2]])
print(f"Predicted Fuel Efficiency: {predicted_efficiency[0]}")
```

### Cost function
- It measures the difference bw model's prediction and actual true y values used for linear regression and written as:

$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)^2
$$

Where:
- model: $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{2}$$
- w: coefficients/weights
- b: bias/y-intercept
- m: total no of training examples (rows)
- i: iterations of x with respect to y
- Objective: minimizes J(w,b)

***Goal:***
- Your goal is to find a model $f_{w,b}(x) = wx + b$, with parameters $w,b$,  which will accurately predict house values given an input $x$. The cost is a measure of how accurate the model is on the training data.

![ML Linear Regression](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/C1_W1_L3_S2_Lecture_b.png)

### Gradient Descent
- Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks by minimizing errors between predicted and actual results. It is used for finding values of parameters w and b that minimize the cost function J.

$$
\begin{align*} \text{repeat}\text{ until convergence: } \lbrace \newline
w = w - \alpha \frac{\partial J(w,b)}{\partial w} \newline 
b = b - \alpha \frac{\partial J(w,b)}{\partial b}  \newline \rbrace \end{align*}
$$

where, parameters $w$, $b$ are updated simultaneously.

The gradient is defined as:

$$
\begin{align*} \frac{\partial J(w,b)}{\partial w} = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}
\newline
\frac{\partial J(w,b)}{\partial b} = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \end{align*}
$$

Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters.


## Polynomial Regression
Polynomial Regression is a powerful technique for modeling non-linear relationships as an nth degree polynomial. While more complex than linear regression, it offers greater flexibility in capturing real-world phenomena that often exhibit curvilinear patterns.

`Equation:` Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ + ... + Œ≤‚ÇôX‚Åø + Œµ

Where:
- Y is the dependent variable
- X is the independent variable
- Œ≤‚ÇÄ, Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô are the coefficients
- n is the degree of the polynomial
- Œµ is the error term

### Visualization
![Polynomial Regression Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/polynomial-regression.png)

`Real-life scenario:` Modeling the growth of a plant over time, where it grows slowly at first, accelerates, and then slows down, capturing the non-linear growth pattern.

### Where to Use
- The relationship between variables is known to be non-linear
- Linear regression doesn't provide a good fit to the data
- You need to model curved relationships
- The data shows clear non-linear trends when plotted


## Logistic Regression
Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1.

`Sigmoid Function Formula:`  $g(z) = \frac{1}{1+e^{-z}}\tag{1}$

- For a logistic regression model, $z = \mathbf{w} \cdot \mathbf{x} + b$

$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x}^{(i)} + b ) $$ 

Where
- ùëî(ùëß) is known as the sigmoid function and it maps all input values to values between 0 and 1
- z = Input to the function (z= w*x + b)
- e = Base of natural log

### Visualization
![Logistic Regression Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/logistic-regression.png)

### Key Characteristics
- Used for classification problems
- Outcome is binary (0 or 1, Yes or No, True or False)
- Produces a S-shaped curve (logistic curve) instead of a straight line

`Real-life scenario:` A bank uses logistic regression to predict the likelihood of a customer defaulting on payments based on attributes like credit score, income, and credit history. This helps automate the credit card approval process efficiently.


## K Nearest Neighbors (KNN)
KNN is a simple, non-parametric algorithm used for classification and regression tasks. It predicts the class or value of a data point by looking at the 'K' nearest data points and using majority voting (for classification) or averaging (for regression).

### Key Steps

1. **Calculate Euclidean Distance**: ‚àö((x2 - x1)¬≤ + (y2 - y1)¬≤)
2. **Select K Nearest Neighbors**:
   The 'K' closest points based on distance (e.g., K=3).
3. **Majority Voting**:
   For K ‚â• 3, the class is decided by majority voting or averaging (for regression).

### Visualization
![KNN Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/knn-visualization.png)

### Applications
- **Classification**:
  Predicts categories (e.g., whether a user will like a movie).
- **Regression**:
  Predicts continuous values (e.g., estimating a user‚Äôs rating for a movie).

### Key Points
- Works for both classification and regression.
- Sensitive to the choice of K and the scale of the data.


## Decision Trees
A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. It splits the data into subsets based on the most significant feature.

### Components
- **Root Node**: Top decision node, represents entire dataset.
- **Internal Nodes**: Decision nodes that splits based on a feature.
- **Leaf Nodes**: Final node that represents the predicted outcome.
- **Branches**: Connections between nodes representing the decision flow.

### Key Concepts:
- **Splitting:** Process of dividing a node into sub-nodes
- **Entropy:** Measure of impurity/disorder in the data
- **Information Gain:** Reduction in entropy after a split
- **Gini Impurity:** Alternative to entropy (for CART algorithm)
- **Pruning:** Process of removing unnecessary branches to prevent overfitting

### Building Process Steps:
1) Calculate entropy (H) of entire dataset

$$
H(S) = S\[+9, -5\] = - \frac{9}{14} \log_2 \left(\frac{9}{14}\right) - \frac{5}{14} \log_2 \left(\frac{5}{14}\right)
$$
- Here +9 = total yes in the dataset output, -5 = total no, 14 = total_dataset_rows

2) Calculate entropy for each value of attributes

$$
H(Sunny) = - \frac{2}{5} \log_2 \left(\frac{2}{5}\right) - \frac{3}{5} \log_2 \left(\frac{3}{5}\right),
H(Rainy) = - \frac{3}{5} \log_2 \left(\frac{3}{5}\right) - \frac{2}{5} \log_2 \left(\frac{2}{5}\right)
$$

3) Calculate Information Gain for each split

$$
IG(S) = H(S) - \left( \frac{5}{14} H(Sunny) \right) - \left( \frac{5}{14} H(Rainy) \right)
$$
- Formula: Entropy (Entire Dataset) - (total_rows_Sunny/total_rows_dataset)Ent Sunny - (total_rows_Rainy/total_rows_dataset)Ent Rainy - ..

4) Choose attribute with highest Information Gain and it will be Root_Node
- Repeat steps 2-4 for each branch until stopping criteria met

### Visualization 
![Decision Trees Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/decision-tree-entropy-visualization.png)

`Real-life scenario:` Predicting whether students can `play football` based on attributes like weather, temperature, humidity, wind.


## Random Forest
Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the class (for classification) or mean prediction (for regression) of the individual trees, and merges their results for more accurate and stable predictions.

### How It Works
1. Create bootstrap dataset from original data by randomly choosing data (repetition is allowed).
2. Build a decision tree for each dataset, using a random subset of features at each split
3. `For classification:` Use majority voting of trees
4. `For regression:` Take the average prediction of all trees

`Real-life scenario:` Predicting if a loan applicant will `default` based on income, credit score, and employment history.

### Visualization 
![Decision Trees Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/Random-Forest-Algorithm.png)

### Pros
- Handles large datasets well.
- Reduces overfitting compared to individual decision trees.
- Works well with both classification and regression tasks.

### Cons
- More complex and slower than a single decision tree.
- Difficult to interpret due to multiple trees.


## Naive Bayes Classification
Naive Bayes is a supervised learning algorithm used for solving classification problems. It is a probabilistic classifier that predicts based on the probability of an object belonging to a particular class.

### Key Concepts
- **Naive**: Assumes that all attributes (features) are independent of each other (which may not always be true in practice).
- **Bayes**: Based on `Bayes' Theorem`, which calculates the conditional probability of an event.

### Bayes' Theorem
P(A|B) = P(B|A) * P(A) / P(B)

Where:
- A, B are Events (yes & no)
- P(A|B) is the posterior probability
- P(B|A) is the likelihood
- P(A) is the prior probability
- P(B) is the marginal likelihood

### Steps:
1) Prior Probability:
    - P(fever=yes) = 7/10
    - P(fever=no) = 3/10

2) Conditional Probability:
```
|       | Yes  | No  |
|--------------|-----|
| Covid | 4/7  | 2/3 |
| Flu   | 3/7  | 2/3 |
```

`Example:` Here yes & no refers to Fever:
- P(yes|flu,Covid) = P(flu/yes) * P(covid/yes) * P(yes)
- P(No|flu,Covid) = P(flu/No) * P(covid/No) * P(No)

### Applications
- Text classification (spam detection, sentiment analysis)
- Face recognition, Weather forecasting, News categorization

`Real-life scenario:` Predicting spam emails based on features like keywords (free, discounts), sender address, and message length. It calculates the probability of an email being spam based on these features.

### Visualization
![Naive Bayes Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/naive-bayes-flowchart-svg.svg)


## Support Vector Machines (SVM)
Support Vector Machine (SVM) is a supervised machine learning algorithm used for both classification and regression tasks. It creates a `hyperplane` to separate data points into different classes, with margins to maximize separation.

### Key Concepts
- **Hyperplane**: A decision boundary that separates the data into classes. In 2D, it's a line; in 3D, it's a plane; in higher dimensions, it's a hyperplane.
- **Support Vectors**: Data points closest to the hyperplane that influence its position and orientation.
- **Margin**: The distance between the hyperplane and the nearest data points from either class.

### How it Works
1. SVM creates two marginal lines parallel to the hyperplane, passing through the nearest data samples of each class.
2. The algorithm seeks to maximize the distance between these marginal lines.
3. The goal is to maximize the margin between the classes.

### Types of SVM
- **Linear SVM**: Used when data can be separated into two classes with a straight line.
- **Non-linear SVM**: Used when data is not linearly separable. Employs kernel functions to transform the data into a higher-dimensional space.

### Kernel Functions
- Transform low-dimensional input space into a higher-dimensional space.
- Convert non-separable problems into separable ones.
- `Common kernel functions:` Polynomial, Sigmoid kernel, Radial Basis Function (RBF)

### Applications
- Text and image classification, Face detection, Financial analysis

`Real-life scenario:` Classifying whether an email is **spam or not** based on features such as frequency of words and links.

### Visualization
![Naive Bayes Flowchart](https://raw.githubusercontent.com/tayyab-balti/AI-Internship/master/Images/svm-email-classification.png)
